作业1：基础流程实现
实现BPE分词器：从零实现字节对编码分词器
Transformer架构：手动实现Transformer模型架构，不能直接调用PyTorch的nn.Transformer或nn.Linear
Adam优化器：从零实现Adam优化算法
训练验证：在TinyStories和OpenWebText数据集上训练模型

作业2：GPU优化
Flash Attention 2：在Triton中实现Flash Attention 2
分布式训练：实现分布式数据并行+优化器分片
系统优化：学习如何使神经语言模型在多台机器的GPU上快速高效运行

作业3：Scaling Law研究
IsoFLOP拟合：使用IsoFLOP拟合Scaling Law
风险模拟：学生获得一个训练API[超参数→损失]和固定计算预算，必须选择提交哪些运行来收集数据点
后台支持：训练API通过在一系列预先计算的运行之间插值来支持

作业4：数据处理
数据转换：将Common Crawl HTML转换为文本
数据过滤：进行质量过滤、有害内容过滤、PII删除
去重处理：删除重复数据
实践意义：这是一项苦差事，却对模型性能至关重要

作业5：模型对齐
监督微调：实现监督微调(SFT)
强化学习：实现专家迭代、GRPO和变体
实践验证：在Qwen 2.5 Math 1.5B上运行RL以提升在MATH上的指标