# 绝对位置编码中的“相对”信息

在经典的 Transformer 架构中，计算 查询向量 $Q_i$ 和 键向量 $K_j$ 的点积时，我们使用 $Q_i = W_Q(E_i + PE_i)$ 和 $K_j = W_K(E_j + PE_j)$，其中 $E$ 是词嵌入， $PE$ 是绝对位置编码。

为了简化分析，我们假设 $W_Q=W_K=I$（恒等矩阵），并且**位置编码直接加到词嵌入上**（即 $Q_i = E_i + PE_i$，$K_j = E_j + PE_j$）。

计算注意力分数（非归一化）时，点积是：
$$\text{Score}_{ij} = Q_i \cdot K_j = (E_i + PE_i) \cdot (E_j + PE_j)$$

展开这个点积，我们得到四项：
$$\text{Score}_{ij} = \underbrace{E_i \cdot E_j}_{\text{a}} + \underbrace{E_i \cdot PE_j}_{\text{b}} + \underbrace{PE_i \cdot E_j}_{\text{c}} + \underbrace{PE_i \cdot PE_j}_{\text{d}}$$

* 项 (a): $E_i \cdot E_j$
    * 这是传统的 **内容-内容** 交互，与位置无关。

* 项 (b) 和 (c): $E_i \cdot PE_j$ 和 $PE_i \cdot E_j$
    * 这是 **内容-绝对位置** 交互。它们允许模型根据 $j$ 的绝对位置来调整 $i$ 的注意力（或反之）。

* 项 (d): $PE_i \cdot PE_j$
    * 这是 **绝对位置-绝对位置** 交互。这一项就是引入相对位置信息的关键。

# 为什么项 (d) 引入相对信息？

对于常见的 正弦/余弦 (Sinusoidal) 绝对位置编码，它的设计特性是：两个位置编码的点积 $PE_i \cdot PE_j$ 往往取决于它们之间的距离 $|i-j|$，而非绝对位置 $i$ 或 $j$。

这是因为正弦/余弦编码使用了以下公式：
$$PE(pos, 2k) = \sin(pos / 10000^{2k/d_{model}})$$
$$PE(pos, 2k+1) = \cos(pos / 10000^{2k/d_{model}})$$

利用三角函数和角和公式 $\cos(\alpha - \beta) = \cos\alpha \cos\beta + \sin\alpha \sin\beta$，我们可以证明：

当 $d_{model}$ 足够大时，位置编码向量之间的点积 $PE_i \cdot PE_j$ 可以近似地被 $\cos(i-j)$ 的组合所表示。

这意味着，项 (d) $PE_i \cdot PE_j$ 鼓励注意力分数去关注 $i$ 和 $j$ 之间的相对距离 $(i-j)$，这正是相对位置编码 (RPE) 的核心思想。

**总结：** 绝对位置编码的数学结构（特别是正弦/余弦编码）使其在点积中自然地包含了 $PE_i \cdot PE_j \approx f(i-j)$ 形式的相对位置偏置。

---

# 引入的“噪声”如何理解？

虽然绝对位置编码的项 (d) 引入了相对位置信息，但它被认为是 **“噪声”** 的原因在于：

## 相对信息是 **非直接、非纯净** 的

* RPE (纯净的相对位置)：纯粹的相对位置编码（如 T5 或 Shaw RPE）**直接**学习一个依赖于相对距离 $k=i-j$ 的偏置项 $B_k$，并将其加到内容点积 $E_i \cdot E_j$ 上。**它只关注 $i-j$。**
$$\text{Score}_{ij} = E_i \cdot E_j + B_{i-j}$$
* APE (混合的相对位置)：在 APE 的点积中，**绝对位置信息** $PE_i$ 和 $PE_j$ 不仅贡献了相对项 $PE_i \cdot PE_j$ (d)，还贡献了**绝对项** $E_i \cdot PE_j$ (b) 和 $PE_i \cdot E_j$ (c)。

“噪声”的本质：模型在训练时，试图从**同时包含绝对位置信息 (b, c)** 和**近似相对位置信息 (d)** 的混合点积中，分离出有用的相对关系。这使得模型对相对关系的捕获变得**间接**且**不够鲁棒**。

## 缺乏平移不变性 (Translational Invariance)

相对位置编码的核心优势是 **平移不变性 (Translational Invariance)**：两个 Token 如果相对距离是 $k$，那么无论它们在序列中的绝对位置 $i$ 和 $j$ 是什么，它们之间的关系都是由 $B_k$ 决定的。

在 APE 的点积中：
$$\text{Score}_{ij} = E_i \cdot E_j + E_i \cdot PE_j + PE_i \cdot E_j + PE_i \cdot PE_j$$

* 如果我们将整个序列平移 $k$ 个位置 (即 $i \to i+k$ 且 $j \to j+k$)，则 $i-j$ 保持不变。
* 但 $PE_i \cdot PE_j$ 确实是近似不变的（这是好的一面）。
* 然而，项 (b) $E_i \cdot PE_j$ 和项 (c) $PE_i \cdot E_j$ 都会改变，因为 $PE_i$ 变成了 $PE_{i+k}$。

这导致 $\text{Score}_{ij}$ 整体不具备平移不变性。模型学习到的“相对”关系被绝对位置信息“污染”了。这种污染就是我们所说的“噪声”，它限制了模型推广相对位置信息的能力。


# 总结

* “混合了相对位置编码”：是指绝对位置编码 $PE_i$ 和 $PE_j$ 的点积项 $PE_i \cdot PE_j$ 利用了正弦/余弦函数的特性，近似地取决于 相对距离 $i-j$，从而隐式地注入了相对位置的偏置。
* “存在噪声”：是指在 APE 的点积中，这种隐式的相对偏置被其他两项 $E_i \cdot PE_j$ 和 $PE_i \cdot E_j$ 引入的绝对位置信息所干扰/污染，导致注意力分数整体上 缺乏平移不变性，使模型难以学习到纯粹、鲁棒的相对位置关系。


您是否想了解 如何通过修改 $Q_i \cdot K_j$ 的展开式来更清晰地分离相对和绝对位置信息（例如在 DeBERTa 和 RoPE 中是如何做的）？