| 阶段| 完成 |章节 | 文章  | 备注| 练习 |
|---|---|---|-------|---|--|
|L1| ✅ | 🧊 注意力机制 | [Transformer](./transformer.md)  | 什么是Transformer，Transformer的原理是什么 | 🔥[transformer.py](exercise/transformer.py)|
|L1| ✅ | 🧊 注意力机制 | [自注意力机制](./自注意力机制.md)  | 什么是自注意力机制，自注意力机制的原理是什么 | 🔥[self_attention.py](exercise/self_attention.py)|
|L1| ✅ | 🧊 注意力机制 | [缩放因子的三个作用](./缩放因子的三个作用.md)  | 为什么需要缩放因子，缩放因子的作用是什么 | |
|L1| ✅ | 🧊 张量 | [张量的连续性](./张量的连续性.md)  | 张量的连续性是什么，为什么需要张量的连续性 | |
|L1| ✅ | 🧊 注意力机制 | [多头注意力机制](./多头注意力机制.md)  | 为什么需要多头注意力机制，什么是多头注意力机制，多头注意力机制的原理是什么 | 🔥[exercise/multi_head_attention.py](exercise/multi_head_attention.py)|
|L1| ✅ | 🧊 位置编码 | [Transformer的位置编码](./Transformer的位置编码.md)  | 理解为什么需要位置编码以及数字位置编码的不足 |
|L1| ✅ | 🧊 位置编码 | [理想Transformer位置编码的五大特性](./理想Transformer位置编码的五大特性.md)  | 理解一个好的位置编码需要满足的特性是什么 |
|L1| ✅ | 🧊 位置编码 | [绝对位置编码如何满足五大特性](./绝对位置编码如何满足五大特性.md)  | 理解SINUSOIDAL POSITION ENCODING是如何满足位置编码的特性 |
|L1| ✅ | 🧊 位置编码 | [绝对位置编码中的相对位置信息如何体现的呢](./绝对位置编码中的相对位置信息如何体现的呢.md) | 理解为什么绝对位置编码中编码了相对位置信息 |
|L1| ✅ | 🧊 位置编码 | [绝对位置编码的相对位置信息的证明](./绝对位置编码的相对位置信息的证明.md) | 证明绝对位置编码中编码了相对位置信息 |
|L1| ✅ | 🧊 位置编码 | [旋转位置编码如何编码相对位置信息](./旋转位置编码如何编码相对位置信息.md) | 旋转位置编码的如何直接编码相对位置信息 |
|L1| ✅ | 🧊 位置编码 | [旋转矩阵的定义与性质](./旋转矩阵的定义与性质.md) | 旋转矩阵的定义与性质 |
|L1| ✅ | 🧊 位置编码 | [旋转矩阵的相位相消证明](./旋转矩阵的相位相消证明.md) | 旋转矩阵的性质证明 |


作业1：基础流程实现
实现BPE分词器：从零实现字节对编码分词器
Transformer架构：手动实现Transformer模型架构，不能直接调用PyTorch的nn.Transformer或nn.Linear
Adam优化器：从零实现Adam优化算法
训练验证：在TinyStories和OpenWebText数据集上训练模型

作业2：GPU优化
Flash Attention 2：在Triton中实现Flash Attention 2
分布式训练：实现分布式数据并行+优化器分片
系统优化：学习如何使神经语言模型在多台机器的GPU上快速高效运行

作业3：Scaling Law研究
IsoFLOP拟合：使用IsoFLOP拟合Scaling Law
风险模拟：学生获得一个训练API[超参数→损失]和固定计算预算，必须选择提交哪些运行来收集数据点
后台支持：训练API通过在一系列预先计算的运行之间插值来支持

作业4：数据处理
数据转换：将Common Crawl HTML转换为文本
数据过滤：进行质量过滤、有害内容过滤、PII删除
去重处理：删除重复数据
实践意义：这是一项苦差事，却对模型性能至关重要

作业5：模型对齐
监督微调：实现监督微调(SFT)
强化学习：实现专家迭代、GRPO和变体
实践验证：在Qwen 2.5 Math 1.5B上运行RL以提升在MATH上的指标