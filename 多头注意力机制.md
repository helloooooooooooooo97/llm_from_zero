# 为什么需要多头注意力机制？

单一的自注意力机制虽然强大，但存在一个局限性：**它只能学习一种类型的注意力模式**。

## 单一注意力的局限性

在自然语言处理中，一个词可能需要同时关注多种不同类型的信息：

- **语法关系**："我" 需要关注动词 "喜欢"
- **语义关系**："篮球" 需要关注 "打" 和 "喜欢"
- **长距离依赖**："篮球" 可能需要关注句子开头的 "我"
- **不同抽象层次**：词级、短语级、句子级的关系

单一的自注意力机制只能学习到一种"平均"的注意力模式，无法同时捕捉这些多样化的关系。

## 什么是多头注意力机制？

多头注意力机制（Multi-Head Attention）通过**并行运行多个自注意力机制**，让模型能够同时学习多种不同类型的注意力模式。

### 核心思想

将输入分成多个"头"（heads），每个头独立学习一种注意力模式，然后将所有头的结果拼接起来。

## 多头注意力机制的原理

### 数学公式

对于 $h$ 个头（heads），多头注意力的计算过程如下：

1. **为每个头生成独立的降维 Q、K、V**：
   $$
   Q_i = X W_Q^{(i)}, \quad K_i = X W_K^{(i)}, \quad V_i = X W_V^{(i)}
   $$
   其中 $i = 1, 2, ..., h$，每个头有各自独立的**降维**权重矩阵。  
   此时 $W_Q^{(i)}, W_K^{(i)}, W_V^{(i)} \in \mathbb{R}^{d_{model} \times d_k}$，$d_k = d_{model}/h$，所以每个头的 QKV 都是 $d_{model} \to d_k$ 的降维变换。

2. **每个头独立计算注意力**：
   $$
   \text{Head}_i = \text{Attention}(Q_i, K_i, V_i) = \text{softmax}\left( \frac{Q_i K_i^T}{\sqrt{d_k}} \right) V_i
   $$

3. **拼接所有头的输出并还原回原始维度**：
   $$
   \text{MultiHead}(Q, K, V) = \text{Concat}(\text{Head}_1, \text{Head}_2, ..., \text{Head}_h) W_O
   $$
   其中 $W_O \in \mathbb{R}^{d_{model} \times d_{model}}$ 是输出投影矩阵，将拼接后的结果从 $h \cdot d_k = d_{model}$ 还原回原始维度。


### 维度设计

#### 维度设计详解

1. **输入维度（$d_{model}$）**  
   这是进入多头注意力层的输入向量的总特征维度。例如在 Transformer-Base 结构中，$d_{model} = 512$，每个输入词会被嵌入成 $512$ 维向量。

2. **头的数量（$h$）**  
   多头注意力机制会将输入特征维度分为 $h$ 份，每一份独立进行注意力计算。例如，$h=8$ 意味着有8个头，每个头提取不同的特征。

3. **每个头的维度（$d_k, d_v$）**  
   通常每个头的 Query、Key、Value 的维度都是 $d_k = d_v = \frac{d_{model}}{h}$。也就是说，原本 $d_{model}$ 维会被等分给 $h$ 个头，每个头只处理 $d_{model}/h$ 维的子空间。例如 $d_{model}=512, h=8$ 时，$d_k=d_v=64$。

#### 为什么这样设计？

- **参数量不变**  
  虽然多头注意力看似有多个独立的注意力结构，但每个头的 QKV 投影矩阵维度为 $d_{model} \times d_k$。$h$ 个头就是 $h \times (d_{model} \times d_k) = d_{model} \times (h \cdot d_k) = d_{model} \times d_{model}$（因为 $h \cdot d_k = d_{model}$），与单头注意力完全一致。

- **每个头专注子空间**  
  每个头只关注原始输入向量的子空间，可以让注意力机制分别捕捉不同类型的信息。例如，部分头专注于语法结构，部分头聚焦于语义联系，还有的头注意长距离依赖等。这里的多头类似于卷积核的概念，每个头是一个卷积核，多个头就是多个卷积核，每个头可以关注不同的特征。

- **特征丰富性提升**  
  多头并行学习，使模型能够同时学习和组合不同类型的注意力分布，从而获得更丰富的表达能力。

- **并行计算提高效率**  
  多个头可以并行运行，硬件上易于加速，同时得到多种“视角”的加权结果，最后拼接并进行一次线性变换还原维度，输出仍为 $d_{model}$。

总结：  
通过这种划分输入与头的方式，实现不增加参数量的前提下，极大提升模型的表达力和捕捉多样关系的能力，是多头注意力的关键优势之一。


### 多头注意力的优势

| 优势 | 说明 |
|------|------|
| **多样性** | 不同头可以学习不同类型的注意力模式（语法、语义、长距离等） |
| **表达能力** | 比单头注意力有更强的表达能力，能捕捉更复杂的关系 |
| **并行性** | 所有头可以并行计算，不增加太多计算开销 |
| **参数效率** | 通过维度分割，参数量与单头相同，但效果更好 |

### 直观理解

想象多头注意力就像有多双"眼睛"同时观察：

- **头1** 可能专注于语法关系（如主谓关系）
- **头2** 可能专注于语义相似性（如同义词）
- **头3** 可能专注于长距离依赖（如代词指代）
- **头4** 可能专注于局部上下文（如短语结构）
- ... 等等

每个头从不同角度理解序列，最后将所有视角的信息融合，得到更丰富的表示。

### 实际效果

在 Transformer 论文中，作者发现：
- 不同头确实学习到了不同的注意力模式
- 有些头关注局部信息，有些头关注全局信息
- 有些头关注语法结构，有些头关注语义关系
- 多头注意力显著提升了模型性能

## 总结

多头注意力机制通过**并行运行多个自注意力子空间**，让模型能够：
1. 同时学习多种注意力模式
2. 从不同角度理解序列
3. 捕捉更丰富的语言关系
4. 在保持参数效率的同时提升表达能力

这是 Transformer 架构成功的关键因素之一。

