# 为什么需要位置编码？

Transformer 的核心是**自注意力机制（Self-Attention）​**，它在处理输入序列时，​**本质上是并行计算所有词之间的关系**，不依赖于词在序列中的位置。

句子 “我 喜 欢 打 篮 球” 和 “篮 球 打 喜 欢 我” 在自注意力看来，只是词的集合不同，但**没有机制知道哪个词先出现、哪个后出现**。

因此，如果不加位置信息，模型无法区分这两个句子的语义差异。

# 为什么不直接用数字编码呢？

| 问题 | 描述 | 举例/影响 |
|------|------|-----------|
| 破坏语义空间、干扰词嵌入 | 直接将整数位置（如+1、+2）加到词向量上，单位不一致、尺度不匹配，扭曲原始语义 | 类比：地图上的 (x,y) 坐标直接加整数“3”，没有意义 |
| 线性增长导致偏向远距离位置 | 线性增长的编号使得后面词的向量更大，Attention 分数易倾向关注靠后的词，误以为越晚出现越重要 | 序列长度1000时，位置1000的编码比位置1大1000倍，模型难以捕捉局部结构 |
| 无法泛化到更长序列 | 训练只看到512以内的位置，测试遇到第513个位置就无法处理 | 位置超出训练最大编号时，模型不认识新编号 |
| 缺乏相对位置信息 | 仅用绝对编号难以反映“词A在词B前面3个位置”这类相对位置信息 | Attention 点积无法直接表达相对顺序关系 |