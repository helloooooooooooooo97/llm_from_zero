# 代码

```python
import torch
import torch.nn as nn

class LayerNorm(nn.Module):
    """
    层归一化（Layer Normalization）

    对每个样本的最后一个特征维度进行归一化处理，使该维度的均值为0、方差为1，有效稳定模型的训练过程。
    应用于Transformer等模型中，能避免因特征分布剧烈变化而导致的训练不稳定。

    公式:
        LayerNorm(x) = γ * (x - μ) / sqrt(σ² + ε) + β
    其中：
        - γ (gamma)：可学习的缩放参数（初始化为1）
        - β (beta)：可学习的偏置参数（初始化为0）
        - μ (mean)：最后一维的均值
        - σ² (variance)：最后一维的方差
        - ε (eps)：保证数值稳定性的小常数（如1e-5）
    """

    def __init__(self, normalized_shape, eps=1e-5):
        """
        初始化层归一化模块
        Args:
            normalized_shape: 要归一化的特征维度大小，通常等于输入的最后一维
            eps: 防止除零的小常数，保证分母稳定
        """
        super().__init__()
        self.normalized_shape = normalized_shape
        self.eps = eps
        # 初始化可学习参数gamma和beta
        # gamma初始为全1（不缩放），beta初始为全0（不偏移）
        self.gamma = nn.Parameter(torch.ones(normalized_shape))  # 可学习缩放参数
        self.beta = nn.Parameter(torch.zeros(normalized_shape))  # 可学习偏置参数

    def forward(self, x):
        """
        前向传播
        Args:
            x: 输入张量，形状为(..., normalized_shape)，即对最后一维做归一化
        Returns:
            归一化并加上仿射变换的结果，与输入x同形状
        """
        # 在最后一维上计算均值，保持形状便于后续广播
        mean = x.mean(dim=-1, keepdim=True)
        # 在最后一维上计算方差（此处为有偏方差，通常和nn.LayerNorm一致用unbiased=False）
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        # 归一化：减去均值再除以标准差（加上eps保证数值安全）
        x_hat = (x - mean) / torch.sqrt(var + self.eps)
        # 应用可学习缩放（gamma）和平移（beta）参数，自动广播到正确形状
        return self.gamma * x_hat + self.beta
```

参数量分析：
- gamma参数和beta参数各有normalized_shape个参数，共2 * normalized_shape个可学习参数 